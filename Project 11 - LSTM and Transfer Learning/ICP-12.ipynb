{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding,LSTM,Dense\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Sentiment.csv') \n",
    "# Keeping only the neccessary columns \n",
    "data = data[['text','sentiment']] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda x: x.lower()) \n",
    "data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x))) \n",
    "for idx, row in data.iterrows(): \n",
    "    row[0] = row[0].replace('rt',' ') \n",
    "max_features = 2000 \n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ') \n",
    "tokenizer.fit_on_texts(data['text'].values) \n",
    "X = tokenizer.texts_to_sequences(data['text'].values) \n",
    "X = pad_sequences(X)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 28, 128)           256000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 196)               254800    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 591       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 511,391\n",
      "Trainable params: 511,391\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196 \n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embed_dim,input_length = X.shape[1])) \n",
    "#model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2)) \n",
    "\n",
    "#These parameters allow for GPU usage for LSTM model, speeding up the training dramatically, otherwise GPU stopped working for this model.\n",
    "model.add(LSTM(lstm_out, activation='tanh',recurrent_activation='sigmoid',recurrent_dropout=0,unroll=False,use_bias=True)) \n",
    "\n",
    "# 3 outputs\n",
    "model.add(Dense(3,activation='softmax')) \n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy']) \n",
    "(model.summary()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorize Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelencoder = LabelEncoder()\n",
    "integer_encoded = labelencoder.fit_transform(data['sentiment']) \n",
    "Y = to_categorical(integer_encoded) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9293, 28)\n",
      "(9293, 3)\n",
      "(4578, 28)\n",
      "(4578, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42) \n",
    "print(X_train.shape)\n",
    "print(Y_train.shape) \n",
    "print(X_test.shape)\n",
    "print(Y_test.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "291/291 [==============================] - 8s 5ms/step - loss: 0.8208 - accuracy: 0.6477\n",
      "Epoch 2/4\n",
      "291/291 [==============================] - 2s 6ms/step - loss: 0.6711 - accuracy: 0.7146\n",
      "Epoch 3/4\n",
      "291/291 [==============================] - 2s 6ms/step - loss: 0.6103 - accuracy: 0.7453\n",
      "Epoch 4/4\n",
      "291/291 [==============================] - 1s 5ms/step - loss: 0.5540 - accuracy: 0.7712\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    model.fit(X_train, Y_train, epochs = 4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144/144 [==============================] - 1s 3ms/step - loss: 0.8030 - accuracy: 0.6651\n",
      "score: 0.80\n",
      "acc: 0.67\n"
     ]
    }
   ],
   "source": [
    "score,acc = model.evaluate(X_test, Y_test) \n",
    "print(\"score: %.2f\" % (score)) \n",
    "print(\"acc: %.2f\" % (acc)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating prediction string and processing the data with previous tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicion_input = ['A lot of good things are happening. We are respected again throughout the world, and that\\'s a great thing.@realDonaldTrump']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicion_input = tokenizer.texts_to_sequences(predicion_input)\n",
    "\n",
    "#Setting the shape to match the input\n",
    "predicion_input = pad_sequences(predicion_input,maxlen=28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.15068953, 0.07452668, 0.7747838 ]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(predicion_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using argmax and class labels to print out the prediction \n",
    "labelencoder.classes_[np.argmax(model.predict(predicion_input))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Spam Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_data = pd.read_csv('spam.csv',encoding='latin-1') \n",
    "# Keeping only the neccessary columns \n",
    "spam_data = spam_data[['v1','v2']] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_data['v2'] = spam_data['v2'].apply(lambda x: x.lower()) \n",
    "spam_data['v2'] = spam_data['v2'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x))) \n",
    "for idx, row in spam_data.iterrows(): \n",
    "    row[0] = row[0].replace('rt',' ') \n",
    "max_features = 2000 \n",
    "tokenizer_spam = Tokenizer(num_words=max_features, split=' ') \n",
    "tokenizer_spam.fit_on_texts(spam_data['v2'].values) \n",
    "X = tokenizer_spam.texts_to_sequences(spam_data['v2'].values) \n",
    "X = pad_sequences(X)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create LSTM model with new embed dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 152, 152)          304000    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 196)               273616    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 394       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 578,010\n",
      "Trainable params: 578,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 152\n",
    "lstm_out = 196 \n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embed_dim,input_length = X.shape[1])) \n",
    "#model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2)) \n",
    "#Faster for GPU\n",
    "model.add(LSTM(lstm_out, activation='tanh',recurrent_activation='sigmoid',recurrent_dropout=0,unroll=False,use_bias=True)) \n",
    "# 2 outputs this time\n",
    "model.add(Dense(2,activation='softmax')) \n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy']) \n",
    "(model.summary()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelencoder = LabelEncoder()\n",
    "integer_encoded_spam = labelencoder.fit_transform(spam_data['v1']) \n",
    "Y = to_categorical(integer_encoded_spam) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3733, 152)\n",
      "(3733, 2)\n",
      "(1839, 152)\n",
      "(1839, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42) \n",
    "print(X_train.shape)\n",
    "print(Y_train.shape) \n",
    "print(X_test.shape)\n",
    "print(Y_test.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "117/117 [==============================] - 2s 11ms/step - loss: 0.1610 - accuracy: 0.9475\n",
      "Epoch 2/10\n",
      "117/117 [==============================] - 1s 11ms/step - loss: 0.0350 - accuracy: 0.9890\n",
      "Epoch 3/10\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.0145 - accuracy: 0.9960\n",
      "Epoch 4/10\n",
      "117/117 [==============================] - 1s 11ms/step - loss: 0.0059 - accuracy: 0.9981\n",
      "Epoch 5/10\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 0.0649 - accuracy: 0.9882\n",
      "Epoch 6/10\n",
      "117/117 [==============================] - 1s 11ms/step - loss: 0.0091 - accuracy: 0.9968\n",
      "Epoch 7/10\n",
      "117/117 [==============================] - 1s 11ms/step - loss: 0.0032 - accuracy: 0.9989\n",
      "Epoch 8/10\n",
      "117/117 [==============================] - 1s 11ms/step - loss: 0.0017 - accuracy: 0.9995\n",
      "Epoch 9/10\n",
      "117/117 [==============================] - 1s 11ms/step - loss: 0.0012 - accuracy: 0.9995\n",
      "Epoch 10/10\n",
      "117/117 [==============================] - 1s 12ms/step - loss: 6.7289e-04 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    model.fit(X_train, Y_train, epochs = 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58/58 [==============================] - 1s 5ms/step - loss: 0.1086 - accuracy: 0.9810\n",
      "score: 0.11\n",
      "acc: 0.98\n"
     ]
    }
   ],
   "source": [
    "score,acc = model.evaluate(X_test, Y_test) \n",
    "print(\"score: %.2f\" % (score)) \n",
    "print(\"acc: %.2f\" % (acc)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running example from https://colab.research.google.com/github/keras-team/keras-io/blob/master/guides/ipynb/transfer_learning.ipynb#scrollTo=rUEK2sB5_tkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a cat/dog dataset with 25000 images\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "#Use a smaller amount of the dataset\n",
    "train_ds, validation_ds, test_ds = tfds.load(\n",
    "    \"cats_vs_dogs\",\n",
    "    # Reserve 10% for validation and 10% for test\n",
    "    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n",
    "    as_supervised=True,  # Include labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (150, 150)\n",
    "\n",
    "train_ds = train_ds.map(lambda x, y: (tf.image.resize(x, size), y))\n",
    "validation_ds = validation_ds.map(lambda x, y: (tf.image.resize(x, size), y))\n",
    "test_ds = test_ds.map(lambda x, y: (tf.image.resize(x, size), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_ds = train_ds.cache().batch(batch_size).prefetch(buffer_size=10)\n",
    "validation_ds = validation_ds.cache().batch(batch_size).prefetch(buffer_size=10)\n",
    "test_ds = test_ds.cache().batch(batch_size).prefetch(buffer_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rotate images so they are slightly different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "data_augmentation = keras.Sequential(\n",
    "    [layers.RandomFlip(\"horizontal\"), layers.RandomRotation(0.1),]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model with imagenet weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 150, 150, 3)]     0         \n",
      "                                                                 \n",
      " sequential_2 (Sequential)   (None, 150, 150, 3)       0         \n",
      "                                                                 \n",
      " rescaling (Rescaling)       (None, 150, 150, 3)       0         \n",
      "                                                                 \n",
      " xception (Functional)       (None, 5, 5, 2048)        20861480  \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 2048)             0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 2049      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,863,529\n",
      "Trainable params: 2,049\n",
      "Non-trainable params: 20,861,480\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model = keras.applications.Xception(\n",
    "    weights=\"imagenet\",  # Load weights pre-trained on ImageNet.\n",
    "    input_shape=(150, 150, 3),\n",
    "    include_top=False,\n",
    ")  # Do not include the ImageNet classifier at the top.\n",
    "\n",
    "# Freeze the base_model\n",
    "base_model.trainable = False\n",
    "\n",
    "# Create new model on top\n",
    "inputs = keras.Input(shape=(150, 150, 3))\n",
    "x = data_augmentation(inputs)  # Apply random data augmentation\n",
    "\n",
    "# Pre-trained Xception weights requires that input be scaled\n",
    "# from (0, 255) to a range of (-1., +1.), the rescaling layer\n",
    "# outputs: `(inputs * scale) + offset`\n",
    "scale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\n",
    "x = scale_layer(x)\n",
    "\n",
    "# The base model contains batchnorm layers. We want to keep them in inference mode\n",
    "# when we unfreeze the base model for fine-tuning, so we make sure that the\n",
    "# base_model is running in inference mode here.\n",
    "x = base_model(x, training=False)\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout\n",
    "outputs = keras.layers.Dense(1)(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "291/291 [==============================] - 42s 129ms/step - loss: 0.1713 - binary_accuracy: 0.9257 - val_loss: 0.0855 - val_binary_accuracy: 0.9690\n",
      "Epoch 2/4\n",
      "291/291 [==============================] - 79s 271ms/step - loss: 0.1092 - binary_accuracy: 0.9541 - val_loss: 0.0760 - val_binary_accuracy: 0.9725\n",
      "Epoch 3/4\n",
      "291/291 [==============================] - 20s 68ms/step - loss: 0.1146 - binary_accuracy: 0.9535 - val_loss: 0.0745 - val_binary_accuracy: 0.9729\n",
      "Epoch 4/4\n",
      "291/291 [==============================] - 19s 66ms/step - loss: 0.0993 - binary_accuracy: 0.9596 - val_loss: 0.0753 - val_binary_accuracy: 0.9733\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.BinaryAccuracy()],\n",
    ")\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    model.fit(train_ds, epochs=4, validation_data=validation_ds)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "435d0e8858e95bf99d49c65e9620e6ce1bbece1b398b9a76f5562c3dd3ad096e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
